import os
import requests
from dotenv import load_dotenv
from requests.auth import HTTPBasicAuth

load_dotenv()
SONARQUBE_URL = os.getenv("SONARQUBE_URL")
SONARQUBE_TOKEN = os.getenv("SONARQUBE_TOKEN")

# === æ“·å–æŒ‡å®šæª”æ¡ˆä¸­å‡ºéŒ¯çš„ç¨‹å¼ç¢¼å€å¡Š ===
def get_code_snippets_from_issues(project_key: str, repo_path: str):
    url = f"{SONARQUBE_URL}/api/issues/search"
    params = {"componentKeys": project_key, "resolved": "false", "ps": 100}
    auth = HTTPBasicAuth(SONARQUBE_TOKEN, "")
    
    response = requests.get(url, auth=auth, params=params)
    issues = response.json().get("issues", [])
    
    snippets = []
    for issue in issues:
        file_path = issue.get("component", "").replace(f"{project_key}:", "")
        full_path = os.path.join(repo_path, file_path)
        line = issue.get("line")

        try:
            with open(full_path, "r", encoding="utf-8") as f:
                lines = f.readlines()
                context = lines[max(0, line-4):line+3]  # å‰å¾Œ 3 è¡Œä½œç‚ºä¸Šä¸‹æ–‡
                snippets.append({
                    "file": file_path,
                    "line": line,
                    "code": "".join(context),
                    "message": issue.get("message", ""),
                    "rule": issue.get("rule", "")
                })
        except Exception as e:
            snippets.append({
                "file": file_path,
                "line": line,
                "code": "âŒ ç„¡æ³•è®€å–åŸå§‹ç¢¼",
                "message": str(e),
                "rule": issue.get("rule", "")
            })

    return snippets

# === çµ¦ GPT ä¿®å¾©ç¨‹å¼ç¢¼ ===
from langchain_openai import ChatOpenAI

# Initialize the language model
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

def fix_code_with_gpt(snippet: dict) -> str:
    if not snippet or not isinstance(snippet, dict):
        return "âŒ snippet æ ¼å¼éŒ¯èª¤ï¼Œè«‹æä¾›æœ‰æ•ˆçš„å­—å…¸è³‡æ–™ã€‚"

    required_keys = ['message', 'rule', 'line', 'code']
    for key in required_keys:
        if key not in snippet:
            return f"âŒ ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{key}"

    prompt = f"""
ä½ æ˜¯ä¸€ä½è³‡æ·±ç¨‹å¼ç¢¼ä¿®å¾©å·¥ç¨‹å¸«ï¼Œè«‹æ ¹æ“šä»¥ä¸‹éŒ¯èª¤è¨Šæ¯èˆ‡ç¨‹å¼ç¢¼å…§å®¹ï¼Œé€²è¡Œ**å¯¦éš›ä¿®æ­£**ï¼ˆåŒ…å«å‘½åéŒ¯èª¤ã€å‹åˆ¥ä¸ä¸€è‡´ã€æœªä½¿ç”¨çš„è®Šæ•¸ç­‰ï¼‰ï¼Œæä¾›ä¿®æ­£å¾Œçš„ç¨‹å¼ç¢¼ç‰ˆæœ¬ã€‚

ğŸš¨ å•é¡Œæè¿°: {snippet['message']}
ğŸ“œ å•é¡Œè¦å‰‡: {snippet['rule']}
ğŸ“„ åŸå§‹ç¢¼ï¼ˆç¬¬ {snippet['line']} è¡Œé™„è¿‘ï¼‰:
{snippet['code']}

è«‹ç›´æ¥å›å‚³å®Œæ•´ä¿®æ­£å¾Œçš„ç¨‹å¼ç¢¼ï¼Œä¸¦ç¢ºä¿ï¼š
1. æœ‰æ ¹æ“šéŒ¯èª¤è¨Šæ¯é€²è¡Œå¯¦éš›ä¿®æ”¹
2. å‘½åè¦å‰‡éŒ¯èª¤éœ€æ›´æ­£ç‚º PEP8 åˆè¦å‘½å
3. ä¸è¦ç•™ä¸‹æœªä½¿ç”¨è®Šæ•¸æˆ–éŒ¯èª¤èªæ³•
4. è«‹å‹¿æ·»åŠ å…¶ä»–èªªæ˜æˆ–è¨»è§£ï¼Œåªå›å‚³ç´”ç¨‹å¼ç¢¼
"""

    try:
        response = llm.invoke(prompt)
        return response.content.strip()
    except Exception as e:
        return f"âŒ GPT å›æ‡‰å¤±æ•—ï¼š{str(e)}"
